# Default values for mims-server-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This is the replica count for model inference server
replicaCount: 2

image:
  repository: iad.ocir.io/bigdatadatasciencelarge/gr-mims-repo/oci-ds-model-server
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: "v2.020823"
  scTag: "v2.091223"
  rmTag: "v1.101423"

imagePullSecrets: 
  - name: ocir-auth-token

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "mmis-sa"

deploy:
  name: "mis-pytorch-v16"
  pipelineId: "NA"
  id: "NA"
  condaEnv: "pytorch20_p39_gpu_v1_6"
  targetEnv: "dev"

# Compute platform types can be CPU / GPU.  This values is used for info. 
# purposes only!
platform:
  type: "GPU"

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  name: "mis-pytorch-v16"
  type: ClusterIP
  # type: LoadBalancer
  port: 80

container:
  port: 8000
  healthCheckUrl: "/healthcheck/"
  delaySeconds: 10
  periodSeconds: 25

ingress:
  enabled: true
  className: ""
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
  hosts:
    - paths:
        - path: /api/v1/
          pathType: Prefix
  tls:
    secretName: tls-secret

# We usually recommend not to specify default resources and to leave this as 
# a conscious choice for the user. This also increases chances charts run on 
# environments with little resources, such as Minikube. If you do want to 
# specify resources, uncomment the following lines, adjust them as necessary,
# and remove the curly braces after 'resources:'.
# *** Important note ***
# GPU's listed under this section should only be assigned to model inference
# servers.
resources:
  limits:
    nvidia.com/gpu: 2
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
